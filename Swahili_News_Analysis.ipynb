{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinyarah/Swahili-News-Classification/blob/main/Swahili_News_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfZCDKuyBTln"
      },
      "source": [
        "#EXPLORING THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-NrvbMtXDWgj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIB5vhZwBbTu"
      },
      "source": [
        "##Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szt3c3K33Z-j"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud ,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJHnl_F36Bi5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "f42a7f2c-ec18-40ee-b61a-6207a1ae6e02"
      },
      "source": [
        "# We load our dataset\n",
        "#test = pd.read_csv(\"/content/Test.csv\")\n",
        "train = pd.read_csv(\"/content/train.csv\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-20c6cab516c4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We load our dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#test = pd.read_csv(\"/content/Test.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1ZBBLkU6Xw5"
      },
      "source": [
        "# Lets load the first 6 rows for each dataset\n",
        "\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnXcZDwcjoiJ"
      },
      "source": [
        "print(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "id": "tl8xEVJe3rOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niZgW8H36kUX"
      },
      "source": [
        "#viewing the top rows of the test dataset\n",
        "#test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXs4Crrx7GNw"
      },
      "source": [
        "#checking for the unique features in the category column\n",
        "train['category'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTVey4MM60Be"
      },
      "source": [
        "#previewing the shape of the dataset\n",
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIGlB5ry6m3-"
      },
      "source": [
        "#checking the datatypes of the datset\n",
        "train.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sl2653N66HN"
      },
      "source": [
        "#previewing the information in the dataset\n",
        "train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GbQD1ed7ouf"
      },
      "source": [
        "# Text pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcKbPllk77GV"
      },
      "source": [
        "#### Lets check for Null Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ-usSeT7ZBd"
      },
      "source": [
        "#checking for the null values\n",
        "train.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJxLvwRk8A0x"
      },
      "source": [
        "#inspecting the presence of duplicated data\n",
        "train.duplicated().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dimflEmCQuGK"
      },
      "source": [
        "#finding missing values\n",
        "test.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOfzijGLQ75l"
      },
      "source": [
        "#checking the presence of duplicated data\n",
        "test.duplicated().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaK9AuCS88PF"
      },
      "source": [
        "#### Convert to Lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5jOrmfr8Hth"
      },
      "source": [
        "# Lets normalise the case of our words to lower case\n",
        "\n",
        "train['content'] = train['content'].str.lower()\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nei3IoJ2ROCd"
      },
      "source": [
        "#normalizing the case of our words to lower case\n",
        "test['content']=test['content'].str.lower()\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6stLf_JU82Cl"
      },
      "source": [
        "#### Removing Punctuation marks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6AbYltoLXg9"
      },
      "source": [
        " # removing the white spaces,commas except alphabet (small and caps)\n",
        " train['content']=train['content'].str.replace(\"[^a-zA-Z]\",\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq6fzWF4LaZZ"
      },
      "source": [
        "#confirming the removal\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn_hhJQPj2Kg"
      },
      "source": [
        "print(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAkYZYceRtG7"
      },
      "source": [
        "#removing white spaces and commas\n",
        "test['content']=test['content'].str.replace(\"[^a-zA-Z]\",\" \")\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9Tf5IorShrA"
      },
      "source": [
        "len(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJtnFXBrv0R8"
      },
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccraznJwv4OR"
      },
      "source": [
        "#importing tokenization libraries\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGtGXxytwinp"
      },
      "source": [
        "type(['content'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LFVF6O--kGy"
      },
      "source": [
        "#assigning the content column to con_tokens\n",
        "con_tokens = train['content']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaG6XsCGH_Oa"
      },
      "source": [
        "word=[]\n",
        "for i in con_tokens:\n",
        "    word.append(word_tokenize(i))\n",
        "    print (con_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyfCcm6a-Fda"
      },
      "source": [
        "len(con_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYkYXgOFo7mP"
      },
      "source": [
        "tokenized_sents = [word_tokenize(i) for i in con_tokens]\n",
        "for i in tokenized_sents:\n",
        "    print (i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "645X04GoAAEr"
      },
      "source": [
        "#confirming tokenization\n",
        "len(tokenized_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSQI3brg085H"
      },
      "source": [
        "#### Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg4ci3vVIVbn"
      },
      "source": [
        "#  swahili stopwords list\n",
        "filter_sent=[]\n",
        "\n",
        "stopwords_swahili=[\"akasema\",\"alikuwa\",\"amesema\",\"alisema\",\"ambayo\",\"ambapo\",\"ambao\",\"baada\",\"basi\",\"bila\",\"cha\",\"chini\",\"hadi\",\"hapo\",\"hata\",\n",
        "                   \"hivyo\",\"hiyo\",\"huku\",\"huo\",\"ili\",\"ilikuwa\",\"juu\",\"kama\",\"karibu\",\"katika\",\"kila\",\n",
        "                   \"kima\",\"kisha\",\"kubwa\",\"kutoka\",\"kutokana\",\"kwani\",\"kuwa\",\"kwa\",\"kwamba\",\"kwenda\",\"kwenye\",\"la\",\n",
        "                   \"lakini\",\"mara\",\"mdogo\",\"mimi\",\"mkubwa\",\"mmoja\",\"moja\",\"muda\",\"mwenye\",\"na\",\n",
        "                   \"naye\",\"ndani\",\"ng\",\"ni\",\"nini\",\"nonkungu\",\"pamoja\",\"pia\",\"sana\",\"sasa\",\"sauti\",\n",
        "                   \"tafadhali\",\"tena\",\"tu\",\"vile\",\"wa\",\"wakati\",\"wake\",\"walikuwa\",\"wao\",\"watu\",\"wengine\",\n",
        "                   \"wote\",\"ya\",\"yake\",\"yangu\",\"yao\",\"yeye\",\"yule\",\"za\",\"zaidi\",\"zake\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qnrAxVndQd3"
      },
      "source": [
        "for i in word:\n",
        "    l=[]\n",
        "    for j in i:\n",
        "        if j not in stopwords_swahili:\n",
        "# This method appends an element to the end of the list\n",
        "            l.append(j)\n",
        "    filter_sent.append(' '.join(l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBEsYQ5U5qM_"
      },
      "source": [
        "train['content'] = filter_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwkAQ1Gu5u0K"
      },
      "source": [
        "# lets preview our dataset\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGXnxrWt52tk"
      },
      "source": [
        "#check news category distribution\n",
        "train.category.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkDyAotB1NwJ"
      },
      "source": [
        "## Bag of words(BoW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIAYyNbX1Sz1"
      },
      "source": [
        "#importing heapq to prioritise most frequent word\n",
        "import heapq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f26RXL1GrN-"
      },
      "source": [
        "# Creating the Bag of Words model\n",
        "#We declare a dictionary to hold our bag of words.\n",
        "#Next we tokenize each sentence to words.\n",
        "#Now for each word in sentence, we check if the word exists in our dictionary.\n",
        "#If it does, then we increment its count by 1. If it doesn’t, we add it to our dictionary and set its count as 1.\n",
        "word2count = {}\n",
        "for data in con_tokens:\n",
        "    words = nltk.word_tokenize(data)\n",
        "    for word in words:\n",
        "        if word not in word2count.keys():\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38mRfLaXUJ7U"
      },
      "source": [
        "print(word2count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvP1vbATGSLd"
      },
      "source": [
        "#identifying the frequently used 1000 words\n",
        "freq_words = heapq.nlargest(1000, word2count, key=word2count.get)\n",
        "freq_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xETPmULPJq97"
      },
      "source": [
        "#we construct a vector, which would tell us whether a word in each sentence is a frequent word or not\n",
        "#X = []\n",
        "#for data in con_tokens:\n",
        "#    vector = []\n",
        "#    for word in freq_words:\n",
        " #       if word in nltk.word_tokenize(data):\n",
        " #           vector.append(1)\n",
        " #       else:\n",
        "  #          vector.append(0)\n",
        " #   X.append(vector)\n",
        "#X = np.asarray(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJnw7ekgMmSA"
      },
      "source": [
        "#print(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTJnRxHabJcx"
      },
      "source": [
        "#TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVgfp2Q3ZExz"
      },
      "source": [
        "# import required module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# create object\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# get tf-df values\n",
        "result = tfidf.fit_transform(con_tokens)\n",
        "\n",
        "# get idf values\n",
        "print('\\nidf values:')\n",
        "for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
        "    print(ele1, ':', ele2)\n",
        "\n",
        "# get indexing\n",
        "print('\\nWord indexes:')\n",
        "print(tfidf.vocabulary_)\n",
        "\n",
        "# display tf-idf values\n",
        "print('\\ntf-idf value:')\n",
        "print(result)\n",
        "\n",
        "# in matrix form\n",
        "print('\\ntf-idf values in matrix form:')\n",
        "print(result.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuApAgkQwNAZ"
      },
      "source": [
        "#LEMMATIZATION\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juxW3jequxel"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr0sB8fOuxbV"
      },
      "source": [
        "# Create WordNetLemmatizer object\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# Performing lemmatization\n",
        "con_tokens = train['content']\n",
        "\n",
        "for words in con_tokens:\n",
        "    print(words , \" : \" , wnl.lemmatize(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Lg897yau3R"
      },
      "source": [
        "#STEMMING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-1TJevUatSf"
      },
      "source": [
        "# importing modules\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "con_tokens = train['content']\n",
        "\n",
        "\n",
        "for w in con_tokens:\n",
        "    print(w, \" : \", ps.stem(w))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNlrSXUbfUNg"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "### Distribution of the categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh3GwU-O67Lw"
      },
      "source": [
        "#Viewing the distribution of the five categories\n",
        "import seaborn as sns\n",
        "sns.countplot(x='category', data=train, order=train['category'].value_counts().sort_values().index)\n",
        "plt.title('Distribution of news categories')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzDPZTfatyRT"
      },
      "source": [
        "# Frequency tables\n",
        "train['News_length'] = train['content'].str.len()\n",
        "print(train['News_length'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwXh8rT7laoj"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHUvRCag05DF"
      },
      "source": [
        "# Lets plot our frequency distribution\n",
        "\n",
        "sns.distplot(train['News_length']).set_title('News length distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvP4EH-ydwSD"
      },
      "source": [
        "## Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC12ZWZ50-mS"
      },
      "source": [
        "# Lets visualize also our content column using word cloud\n",
        "# Our column is grouped in five(Kitaifa, kimataifa, biashara, burudani, michezo)\n",
        "# lets create a function to create wordcloud\n",
        "def create_wordcloud(words):  # takes input as string\n",
        "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(words)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp1m_9Ve5QWp"
      },
      "source": [
        "### Wordcloud - Kitaifa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK4_dpJA4NHm"
      },
      "source": [
        "# wordcloud for category Kitaifa\n",
        "subset=train[train.category==\"Kitaifa\"]\n",
        "# list of sentences\n",
        "text=subset.content.values\n",
        "# convert list of sentences into a paragraph of sentences\n",
        "words =\" \".join(text)\n",
        "create_wordcloud(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jjfFiuN5Xwe"
      },
      "source": [
        "#### Kimataifa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tmGDby25PEg"
      },
      "source": [
        "# wordcloud for category Kitaifa\n",
        "subset=train[train.category==\"Kimataifa\"]\n",
        "# list of sentences\n",
        "text=subset.content.values\n",
        "# convert list of sentences into a paragraph of sentences\n",
        "words =\" \".join(text)\n",
        "create_wordcloud(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxGNYBBg5kCO"
      },
      "source": [
        "####Biashara"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbrb9NZ45rX2"
      },
      "source": [
        "# wordcloud for category Kitaifa\n",
        "subset=train[train.category==\"Biashara\"]\n",
        "# list of sentences\n",
        "text=subset.content.values\n",
        "# convert list of sentences into a paragraph of sentences\n",
        "words =\" \".join(text)\n",
        "create_wordcloud(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCh0Ed476Dut"
      },
      "source": [
        "####Michezo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adG22jCB5xNP"
      },
      "source": [
        "#michezo word cloud\n",
        "subset=train[train.category==\"michezo\"]\n",
        "text=subset.content.values\n",
        "words =\" \".join(text)\n",
        "create_wordcloud(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsZbkzD16h3K"
      },
      "source": [
        "####Burudani"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvVM89ms6MDa"
      },
      "source": [
        "#burudani word cloud\n",
        "subset=train[train.category==\"Burudani\"]\n",
        "text=subset.content.values\n",
        "words =\" \".join(text)\n",
        "create_wordcloud(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbyHtku5-E8A"
      },
      "source": [
        "## Numerical Variable\n",
        "\n",
        "\n",
        "### Measures of Central Tendency\n",
        "\n",
        "####a)Mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gcCnJ2fAiA1"
      },
      "source": [
        "# Finding the mean of news length\n",
        "train[\"News_length\"].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkz-JMvD_-l4"
      },
      "source": [
        "##Measures of Dispersion/ Variabilty/Spread\n",
        "\n",
        "\n",
        "####a) Standard Deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvfTmZyTBQPt"
      },
      "source": [
        "# Lets check the standard deviation of the news length\n",
        "train[\"News_length\"].std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN1q_9vrlUek"
      },
      "source": [
        "#having a look at the number of characters present in each sentence\n",
        "train['content'].str.len().hist()\n",
        "plt.xlabel('No. of sentences')\n",
        "plt.ylabel('No. of characters')\n",
        "plt.title('No. of characters in a sentence')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr6Vv4_xBcSY"
      },
      "source": [
        "#Up next, let’s check the average word length in each sentence.\n",
        "\n",
        "train['content'].str.split().\\\n",
        "   apply(lambda x : [len(i) for i in x]). \\\n",
        "   map(lambda x: np.mean(x)).hist()\n",
        "plt.xlabel('Avg. word length')\n",
        "plt.ylabel('No. of sentences')\n",
        "plt.title('Average word per sentence')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHWJwo5bLJG4"
      },
      "source": [
        "#Implementing the Solution\n",
        "##Preparing our data for modelling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkSQ0GLTfs43"
      },
      "source": [
        "#dropping News_length and id since we didn't need them for modeling\n",
        "train.drop(['News_length'], axis = 1, inplace = True)\n",
        "train.drop(['id'], axis = 1, inplace = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drb75Y9Enwgu"
      },
      "source": [
        "test.drop(['swahili_id'], axis = 1, inplace = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc0NQRK6dxei"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidfvectorizer = TfidfVectorizer()\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import hamming_loss, accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from yellowbrick.model_selection import validation_curve\n",
        "import statsmodels.api as sm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntxbeAknfllP"
      },
      "source": [
        "label_encoder = preprocessing.LabelEncoder()\n",
        "train['category']= label_encoder.fit_transform(train['category'])\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBC3A1YCCnpG"
      },
      "source": [
        "#splitting the independent and dependent variable\n",
        "X = train[\"content\"]\n",
        "y = train['category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AekRo7m2f3Dl"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS5icXfMC2Tw"
      },
      "source": [
        "# initializing TfidfVectorizer\n",
        "vetorizar = TfidfVectorizer(max_features=3000, max_df=0.85,ngram_range=(1,2))\n",
        "# fitting the tf-idf on the given data\n",
        "vetorizar.fit(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWV5Ov4BC7xL"
      },
      "source": [
        "# splitting the data to training and testing data set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "# transforming the data\n",
        "X_train_tfidf = vetorizar.transform(X_train)\n",
        "X_test_tfidf = vetorizar.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMsjY60EEgvq"
      },
      "source": [
        "print(X_train_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECmsMPrF-7fl"
      },
      "source": [
        "# Random over_sampling Imbalanced Datasets(Smote)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rxasq9U94d7"
      },
      "source": [
        "#SMOTE-Oversampling technique used to create a balance between the minority class and the majority class\n",
        "!pip install imblearn\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af0r1XPC96df"
      },
      "source": [
        "smote = SMOTE()\n",
        "features_train_smote, labels_train_smote = smote.fit_sample(X_train_tfidf.astype(\"float\"),y_train)\n",
        "from collections import Counter\n",
        "print('Before SMOTE :' , Counter(y_train))\n",
        "print('After SMOTE :' , Counter(labels_train_smote))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa9dLDc5zXD-"
      },
      "source": [
        "##Multinomial naive bayes\n",
        "####After SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HKGl6c7-C1Z"
      },
      "source": [
        "# implementing Multinomial naive bayes after performing SMOTE\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "model = MultinomialNB()\n",
        "model.fit(features_train_smote,labels_train_smote )\n",
        "model_predictions = model.predict(features_train_smote)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7SJpxb7-J-w"
      },
      "source": [
        "#the accuracy of our model\n",
        "print('Accuracy: ', accuracy_score(labels_train_smote, model_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jKknoo0-QLQ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "report= classification_report(labels_train_smote, model_predictions)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y20pob_Fzv9G"
      },
      "source": [
        "###Before SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nNldW0FzvHd"
      },
      "source": [
        "#implementing naive bayes before SMOTE\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "model_predictions = model.predict(X_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVJ8pFZK0J2H"
      },
      "source": [
        "#the accuracy of our model\n",
        "print('Accuracy: ', accuracy_score(y_test, model_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTASRwJK0UTi"
      },
      "source": [
        "#evaluating the model\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "report= classification_report(y_test, model_predictions)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xrgAaOZVfFy"
      },
      "source": [
        "#K Means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXVQzDFEmcYq"
      },
      "source": [
        "import matplotlib.cm as cm\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX0Rh80sNwkG"
      },
      "source": [
        "tfidf = TfidfVectorizer(\n",
        "    min_df = 5,\n",
        "    max_df = 0.95,\n",
        "\n",
        "    max_features = 3000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkJdkK9ikJ2w"
      },
      "source": [
        "tfidf.fit(train.content)\n",
        "text = tfidf.transform(train.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6iQJpb-N5xd"
      },
      "source": [
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6dcvVyynNxl"
      },
      "source": [
        "#One simple approach is to plot the SSE for a range of cluster sizes.\n",
        "# We look for the \"elbow\" where the SSE begins to level off.\n",
        "#MiniBatchKMeans introduces some noise so we raised the batch and init sizes higher\n",
        "def find_optimal_clusters(data, max_k):\n",
        "    iters = range(2, max_k+1, 2)\n",
        "\n",
        "    sse = []\n",
        "    for k in iters:\n",
        "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(text).inertia_)\n",
        "        print('Fit {} clusters'.format(k))\n",
        "\n",
        "    f, ax = plt.subplots(1, 1)\n",
        "    ax.plot(iters, sse, marker='o')\n",
        "    ax.set_xlabel('Cluster Centers')\n",
        "    ax.set_xticks(iters)\n",
        "    ax.set_xticklabels(iters)\n",
        "    ax.set_ylabel('SSE')\n",
        "    ax.set_title('SSE by Cluster Center Plot')\n",
        "\n",
        "find_optimal_clusters(text, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6s-yzAppWg2"
      },
      "source": [
        "clusters = MiniBatchKMeans(n_clusters=6, init_size=1024,\n",
        "                           batch_size=2048,\n",
        "                           random_state=20).fit_predict(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcLmi-Pspg_x"
      },
      "source": [
        "#Here we plot the clusters generated by our KMeans operation.\n",
        "# One plot uses PCA which is better at capturing global structure of the data.\n",
        "#The other uses TSNE which is better at capturing relations between neighbors\n",
        "def plot_tsne_pca(text, labels):\n",
        "    max_label = max(labels)\n",
        "    max_items = np.random.choice(range(text.shape[0]), size=3500, replace=False)\n",
        "    labels = np.array(['Biashara', 'Kimataifa', 'Kitaifa', 'Michezo', 'Burudani'])\n",
        "\n",
        "    pca = PCA(n_components=2).fit_transform(text[max_items,:].todense())\n",
        "    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(text[max_items,:].todense()))\n",
        "\n",
        "\n",
        "    idx = np.random.choice(range(pca.shape[0]), size=400, replace=False)\n",
        "    label_subset = labels[max_items]\n",
        "    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n",
        "\n",
        "    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n",
        "    ax[0].set_title('PCA Cluster Plot')\n",
        "\n",
        "    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n",
        "    ax[1].set_title('TSNE Cluster Plot')\n",
        "\n",
        "plot_tsne_pca(text, clusters)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5dsPiuuqczc"
      },
      "source": [
        "def get_top_keywords(text, clusters, labels, n_terms):\n",
        "    df = pd.DataFrame(text.todense()).groupby(clusters).mean()\n",
        "\n",
        "    for i,r in df.iterrows():\n",
        "        print('\\nCluster {}'.format(i))\n",
        "        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n",
        "\n",
        "get_top_keywords(text, clusters, tfidf.get_feature_names(), 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PSgeT6xMsmD"
      },
      "source": [
        "#Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrXVB1czHAPX"
      },
      "source": [
        "# Instantiating our model\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC(kernel=\"linear\")\n",
        "\n",
        "# fitting the model\n",
        "svc.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIrcouzoMqK8"
      },
      "source": [
        "# assessing the performance\n",
        "\n",
        "print(\"General performance: \",svc.score(X_test_tfidf, y_test))\n",
        "\n",
        "c_score_svc = cross_val_score(estimator=svc, X=X_train_tfidf, y=y_train, scoring='accuracy', cv=5)\n",
        "\n",
        "print(\"Cross validated model performance: \", c_score_svc.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DykjZH62xsC"
      },
      "source": [
        "#Hyperparameter tuning\n",
        "\n",
        "clf = GridSearchCV(estimator=SVC(),\n",
        "                   param_grid={'kernel' : ['rbf',\"linear\", 'sigmoid'],\n",
        "                               'gamma' : ['scale', 'auto'],\n",
        "                               'C' : np.arange(1, 10, 2)},\n",
        "                   scoring='accuracy',\n",
        "                   cv=5)\n",
        "\n",
        "clf.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joM08Za45DYJ"
      },
      "source": [
        "#evaluating the parameters\n",
        "compare = pd.DataFrame(clf.cv_results_)\n",
        "compare = compare[['param_gamma', 'param_kernel', 'param_C', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False)\n",
        "compare.reset_index(drop=True, inplace=True)\n",
        "compare"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBcB0peuylER"
      },
      "source": [
        "#CHALLENGING THE SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaN6Bg-o8z8j"
      },
      "source": [
        "## Xgboost Model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###Boosting models are another type of ensemble models part of tree based models. Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV5puwTENeii"
      },
      "source": [
        "#importing the libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "import scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnOnUHmxtlFZ"
      },
      "source": [
        "#using countvectorizor to transform text into a vector\n",
        "cv = CountVectorizer(max_features=5000, encoding=\"utf-8\",\n",
        "      ngram_range = (1,3),\n",
        "      token_pattern = \"[A-Za-z_][A-Za-z\\d_]*\")\n",
        "trans = cv.fit_transform(train['content'].values)\n",
        "labels = train['category']\n",
        "xgb=XGBClassifier\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChgOFa09MzzB"
      },
      "source": [
        "#splitting X and y\n",
        "X = scipy.sparse.hstack([trans])\n",
        "y = labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2E7PV35ND3n"
      },
      "source": [
        "#getting the train and test splits\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv0K2_sGNcM9"
      },
      "source": [
        "#parameters of the model\n",
        "xgb_model = xgb(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bG8ZymRNibG"
      },
      "source": [
        "#making predictions\n",
        "xgb_prediction = xgb_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fbu3hewNsr4"
      },
      "source": [
        "#evaliating the model performance\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "\n",
        "print('training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
        "print('validation score:', f1_score(y_test, xgb_model.predict(X_test), average='macro'))\n",
        "print(classification_report(y_test, xgb_prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Thv_ZDw7aR"
      },
      "source": [
        "#creating an array of the vector\n",
        "X = cv.fit_transform(train['content']).toarray()\n",
        "y = train['category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2D1CubtxhUt"
      },
      "source": [
        "#displaying the array in a dataframe\n",
        "count_df = pd.DataFrame(labels, columns=cv.get_feature_names())\n",
        "count_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOqKOLmZW0Pb"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbTagS2WW8aH"
      },
      "source": [
        "We then preprocess the training and validation datasets into the format expected by the selected pretrained model (in this case DistilBERT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyvu3i9nW4kJ"
      },
      "source": [
        "# Reload our test dataset\n",
        "\n",
        "#test = pd.read_csv(\"/content/Test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujHvL28ZYuD7"
      },
      "source": [
        "test.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nde1nSOKYytD"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2VBD_ayY7vd"
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR8EmvpXY-r5"
      },
      "source": [
        "test.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbtJcI6XZEeW"
      },
      "source": [
        "validation_set = test\n",
        "#  converting our test dataset to lowercase\n",
        "validation_set[\"content\"] = validation_set[\"content\"].str.lower() # Val_set it means validation set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfbbqvyYFduS"
      },
      "source": [
        "Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfQybQHPFR-_"
      },
      "source": [
        "#  removal of punctuation marks\n",
        "validation_set['content']=validation_set['content'].str.replace(\"[^a-zA-Z]\",\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HimSYJApZgry"
      },
      "source": [
        "#  previewing our dataset to check if it was converted to lowercase\n",
        "validation_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6XVzcWDUojo"
      },
      "source": [
        "\n",
        "def processText(tweet):\n",
        "\n",
        "  tweet = tweet.lower()                #convert text to lower-case\n",
        "  tweet = re.sub('â€˜','',tweet)    # remove the text â€˜ which appears to occur flequently\n",
        "  tweet = re.sub('â€™','',tweet)    # remove the text â€™ which appears to occur flequently\n",
        "\n",
        "\n",
        "  tweet = word_tokenize(tweet)\n",
        "  return ' '.join(tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4re-gBkZsL-"
      },
      "source": [
        "# lets apply both training and test set to processtext function\n",
        "\n",
        "train['content'] = train['content'].apply(processText)\n",
        "validation_set['content'] = validation_set['content'].apply(processText)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt--mfjGEsLz"
      },
      "source": [
        "validation_set['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41GXoIhx7BEs"
      },
      "source": [
        "valid_pred_to = np.zeros((len(validation_set),5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3AH_-BR86KL"
      },
      "source": [
        "#install ktrain wrapper to help us build and deploy our model\n",
        "!pip install ktrain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twQOuKW_m266"
      },
      "source": [
        "#importing libraries needee for our modelling with bert\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.utils import class_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS7ZxHl-8xeQ"
      },
      "source": [
        "#import important API for text\n",
        "# import and use text as ktrain\n",
        "import tensorflow as tf\n",
        "import ktrain\n",
        "from ktrain import text\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvYg0-tgWgpI"
      },
      "source": [
        "# the classweight is used to assign few label to have high loss compared to other\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(train['category']), train['category'])\n",
        "\n",
        "class_weight_dict = dict(enumerate(class_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShnWOv5D7weE"
      },
      "source": [
        "# appply both training and test set to processtext function\n",
        "\n",
        "train['content'] = train['content'].apply(processText)\n",
        "validation_set['content'] = validation_set['content'].apply(processText)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfMMjYi6mJWa"
      },
      "source": [
        "TRANSFORMERS\n",
        "\n",
        "\n",
        "The core idea behind the Transformer model is self-attention—the ability to attend to different positions of the input sequence to compute a representation of that sequence. Transformer creates stacks of self-attention layers and is explained below in the sections Scaled dot product attention and Multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRk2QE96-ehL"
      },
      "source": [
        "#from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "#loading our pretrained model from hugging face\n",
        "MODEL_NAME = 'bert-base-multilingual-uncased'\n",
        "\n",
        "\n",
        "t = text.Transformer(MODEL_NAME, maxlen= 128,  class_names=['kitaifa','michezo','biashara','kimataifa','burudani'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_woB1ku9mlK"
      },
      "source": [
        " # the parameter of are interest_validation_accuracy and training should stop\n",
        " #if validation accuracy is below best value for 3 else which it continues\n",
        "es = EarlyStopping(monitor='val_accuracy', patience= 3 , verbose=1, restore_best_weights=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YWLpgxt9mbr"
      },
      "source": [
        "seed = 42\n",
        "n_folds = 10\n",
        "\n",
        " # cros validation folds by running 10 folds it will guarantee the best results from developed model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI_29_y09x94"
      },
      "source": [
        "skf = StratifiedKFold(n_splits= n_folds, random_state=seed, shuffle=False)  # stratified for balanced sampling of training sample\n",
        "\n",
        "n = 0  # Sometimes some folds produced worse results and then the model is skipped. n will guarantee the average is divided with only episodes contributing to the results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnbYH3no9x5L"
      },
      "source": [
        "for train_index, test_index in skf.split(train['content'], train['category']):\n",
        "\n",
        " #splitting our data into trainning set and test set for our modelling\n",
        "    x_train, x_test = list(train.loc[train_index,'content']), list(train.loc[test_index,'content'])\n",
        "    y_train, y_test = np.asarray(train.loc[train_index,'category']), np.asarray(train.loc[test_index,'category'])\n",
        " #preprocessing our splitted data before modelling\n",
        "    trn = t.preprocess_train(x_train, y_train)\n",
        "    val = t.preprocess_test(x_test, y_test)\n",
        "\n",
        "    model = t.get_classifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceVn35oC9xy0"
      },
      "source": [
        "    learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size= 6)\n",
        "\n",
        "    history = learner.fit(1e-5, 10, cycle_len=1, cycle_mult=2,\n",
        "                          class_weight= class_weight_dict,\n",
        "                          callbacks=[es], checkpoint_folder='/tmp')\n",
        "    learner.validate(class_names=t.get_classes())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhuwigGkOlcx"
      },
      "source": [
        "  if max(history.history['val_accuracy']) < 0.8:\n",
        "       # I used any model for testing set if max(history) of validation accuracy is above or equal to 80% else continue and other CV\n",
        "      continue\n",
        "\n",
        "    # make inference if the above condition is met\n",
        "\n",
        "    predictor = ktrain.get_predictor(learner.model,preproc=t )\n",
        "    data = validation_set['content']\n",
        "    data = np.asarray(data)\n",
        "    print(predictor.get_classes())\n",
        "    pred = predictor.predict(data,return_proba=True)\n",
        "    n = n+1\n",
        "\n",
        "    valid_pred_ro += pred\n",
        "\n",
        "valid_pred_ro /= n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T4bhLQQ_J5b"
      },
      "source": [
        "#creating a dataframe that stores the values of our output\n",
        "OUT_df = pd.DataFrame(valid_pred_ro, columns= ['biashara', 'burudani', 'kimataifa', 'kitaifa', 'michezo'])\n",
        "\n",
        "df_data_out = OUT_df[['kitaifa','michezo','biashara','kimataifa','burudani']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86Ia0u_4_b5S"
      },
      "source": [
        "SUB_FILE_NAME = 'submission_ro.csv'\n",
        "sub_df.to_csv(SUB_FILE_NAME, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n81_pTsX_N2z"
      },
      "source": [
        "\n",
        "sub_df = pd.DataFrame(valid_pred_ro, columns= ['biashara', 'burudani', 'kimataifa', 'kitaifa', 'michezo'])  # generate dataframe to store results\n",
        "sub_df['swahili_id'] = validation_set['swahili_id']\n",
        "\n",
        "sub_df = sub_df[['swahili_id','kitaifa','michezo','biashara','kimataifa','burudani']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMka4U2jz87m"
      },
      "source": [
        "# # parameter of interest validation accuracy and training should stop if validation accuracy is below best value for 3 consequetive episode\n",
        "es = EarlyStopping(monitor='val_accuracy', patience= 3 , verbose=1, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIlMcbLxQHd7"
      },
      "source": [
        "sub_df = pd.DataFrame(valid_pred_ro, columns= ['biashara', 'burudani', 'kimataifa', 'kitaifa', 'michezo'])  # generate dataframe to store results\n",
        "\n",
        "\n",
        "sub_df = sub_df[['kitaifa','michezo','biashara','kimataifa','burudani']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN6BnJZQQL1L"
      },
      "source": [
        " # print results\n",
        "sub_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWy1UPf-QYE9"
      },
      "source": [
        "SUB_FILE_NAME = 'submission_ro.csv'\n",
        "sub_df.to_csv(SUB_FILE_NAME, index=False)   # save output in the kaggle output file"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}